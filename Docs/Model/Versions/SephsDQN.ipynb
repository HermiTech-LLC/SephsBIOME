{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e428c8",
   "metadata": {},
   "source": [
    "# SephsBiome Adaptive Learning Model using Deep Q-Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the SephsBiome project's advanced learning model, where we leverage the robust capabilities of Deep Q-Networks (DQNs) to facilitate adaptive, intelligent behaviors in complex environments. This notebook serves as a foundational piece in our project's journey towards integrating deep reinforcement learning with evolutionary algorithms, setting a new standard for autonomous learning and adaptation.\n",
    "\n",
    "### The Role of Reinforcement Learning in SephsBiome\n",
    "\n",
    "In the realm of SephsBiome, reinforcement learning is not just a tool; it's a cornerstone that underpins our model's ability to interact with, learn from, and adapt to dynamic environments. The incorporation of DQNs elevates this process, offering a sophisticated approach to handle high-dimensional data and complex decision-making scenarios.\n",
    "\n",
    "### Objective of This Notebook\n",
    "\n",
    "In this notebook, we aim to:\n",
    "1. Introduce the concepts and mechanics of Deep Q-Networks.\n",
    "2. Demonstrate the implementation of a DQN model.\n",
    "3. Showcase the model's training and evaluation within a sample environment representative of the challenges faced in the SephsBiome project.\n",
    "\n",
    "Let's embark on this journey by setting up our environment and delving into the world of DQNs, a pivotal element in the evolution of the SephsBiome project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa3b3a2-01cf-4d0b-918c-3caa03d87ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in /private/var/containers/Bundle/Application/04D0AA1E-524A-4059-A473-1C7AA4A973D1/Carnets-sci.app/Library/lib/python3.11/site-packages (0.26.2)\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym tensorflow numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d344156c-4102-423c-85b2-393d82f4c6f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3e4f7",
   "metadata": {},
   "source": [
    "### Setting Up the Environment\n",
    "We will use a standard environment for our demonstration. Let's initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb9f3a-f109-4d3c-b7f3-395073806eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a04de-9e42-42ee-b458-a0f8edc43309",
   "metadata": {},
   "source": [
    "With the environment set up, we can now proceed to discuss the components of the DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc211324-3bf6-4b26-ba48-77bcc6f85690",
   "metadata": {},
   "source": [
    "## DQN Components\n",
    "In this section, we will explore the key components of a Deep Q-Network.\n",
    "### Neural Network Architecture\n",
    "The neural network in DQN acts as a function approximator for our Q-value. For our CartPole example, we'll use a simple network with fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237f3986-4273-4091-b3ab-bef490864b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, state_size)))  # Adjust the input shape as per the BipedalWalker environment\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))  # Ensure action size matches the BipedalWalker environment\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7eb541-c336-41c6-a718-165c00eb90a1",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "Experience replay allows our DQN to learn from past experiences, stored in a replay buffer. This helps in stabilizing the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e5313-ffd0-4607-96db-10abcbf28222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = collections.deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        minibatch = random.sample(self.buffer, batch_size)\n",
    "        return map(np.array, zip(*minibatch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cf034-7d87-4a35-a1ff-253f40477bba",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation\n",
    "A key challenge in RL is the trade-off between exploration (trying new things) and exploitation (using known information). This is often managed using an Îµ-greedy strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adba48a-2bc2-4de4-b602-dc5b93963b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    else:\n",
    "        q_values = model.predict(state)\n",
    "        return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992e956-f6a5-4449-8a45-9dfabc84193c",
   "metadata": {},
   "source": [
    "With these components in place, we are ready to build and train our DQN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e963a-b37e-4686-b757-b483d003e929",
   "metadata": {},
   "source": [
    "## Implementing DQN\n",
    "Now that we have discussed the components of DQN, let's implement it.\n",
    "### Building the DQN Model\n",
    "Using the function `build_model` we defined earlier, we can create our DQN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acac72-4c60-4678-8981-4efce6208157",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(state_size, action_size)\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942909a-3163-4ea9-b2e4-9d1e4747589c",
   "metadata": {},
   "source": [
    "### Defining the Replay Buffer\n",
    "We instantiate our ReplayBuffer class for storing and sampling experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e3265-90a0-4756-838e-c2a827ae1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a31ed-a576-4aee-8963-be3f3b79137c",
   "metadata": {},
   "source": [
    "### Setting Up the Environment\n",
    "We have already initialized our environment in the Setup section. We will now define additional parameters for our DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb0c83-2440-4eed-911a-ee420375a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0          # Exploration rate\n",
    "epsilon_min = 0.01     # Minimum exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate for exploration\n",
    "batch_size = 64        # Batch size for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83985e6d-77d7-4275-be91-38ef8c9bb887",
   "metadata": {},
   "source": [
    "With the model, replay buffer, and environment set up, we are ready to train our DQN agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b2c1f-3d50-49f0-80c7-6a6e8da6ac0c",
   "metadata": {},
   "source": [
    "## Training the DQN Agent\n",
    "Training a DQN agent involves interacting with the environment and using the gathered experiences to improve our policy.\n",
    "### Training Loop\n",
    "Below is the main loop for training our DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64bef8-8b51-492f-b8e3-7f2867bd4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000  # Total number of episodes for training\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    for time in range(500):\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if len(replay_buffer.buffer) > batch_size:\n",
    "            experiences = replay_buffer.sample(batch_size)\n",
    "            \n",
    "# Extracting experiences\n",
    "states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "# Predicting Q-values for the current states and next states\n",
    "q_values = model.predict(states)\n",
    "next_q_values = model.predict(next_states)\n",
    "\n",
    "# Q-Learning update rule\n",
    "target_q_values = rewards + gamma * np.max(next_q_values, axis=1) * (1 - dones)\n",
    "\n",
    "# Preparing target and enabling gradient update only for the chosen actions\n",
    "targets_full = q_values\n",
    "indices = np.arange(batch_size)\n",
    "targets_full[indices, actions] = target_q_values\n",
    "\n",
    "# Performing a gradient descent step\n",
    "model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {e}/{num_episodes}, Score: {total_reward}\")\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa87d3d",
   "metadata": {},
   "source": [
    "### Tracking Learning Progress\n",
    "We can track the agent's learning progress by plotting the total rewards obtained in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe531b-6ddc-4dc4-b33f-1f2b200e49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(num_episodes)], [total_rewards[i] for i in range(num_episodes)])\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e987de-fe12-467a-99af-9c2e6efccf85",
   "metadata": {},
   "source": [
    "After training, we can evaluate the performance of our DQN agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7644631-590a-46f3-9885-7b2b6fda8b41",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization\n",
    "Once the DQN agent is trained, it's important to evaluate its performance.\n",
    "### Evaluating the Trained Model\n",
    "We can test the trained model by running it on the environment without exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9958d6-cfd5-4e2f-bc21-aa0b7d3634ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(10):  # Test for 10 episodes\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    for time in range(500):\n",
    "        action = np.argmax(model.predict(state)[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(f\"Test Episode: {e}, Score: {total_reward}\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a7b20-27f3-4d4d-a42c-16d96eb52c3d",
   "metadata": {},
   "source": [
    "### Visualizing Performance Metrics\n",
    "Plotting the rewards or other metrics over time can give insights into the learning process and performance.\n",
    "```python\n",
    "# Plotting code for performance metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f51054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards per Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SEPHDQN_model.h5')  # Save the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
