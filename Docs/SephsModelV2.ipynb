{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5cd639c6",
      "metadata": {
        "id": "5cd639c6"
      },
      "source": [
        "\n",
        "# Neural Network Model Development\n",
        "\n",
        "This notebook demonstrates the development of a custom neural network using TensorFlow and Keras, focusing on good coding practices and clear documentation.\n",
        "\n",
        "### Library Imports\n",
        "All necessary libraries are imported here for better organization.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmovAo9dl9j9",
        "outputId": "a842f53d-bd37-47f1-e54a-959d520b7c9e"
      },
      "id": "nmovAo9dl9j9",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3979fb8e",
      "metadata": {
        "id": "3979fb8e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81124325",
      "metadata": {
        "id": "81124325"
      },
      "source": [
        "\n",
        "### Global Variables\n",
        "Defining any constants and global variables used throughout the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "743a72a4",
      "metadata": {
        "id": "743a72a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Adjust these parameters as needed for your model\n",
        "seq_length = 128\n",
        "d_model = 512\n",
        "num_classes = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428eb201",
      "metadata": {
        "id": "428eb201"
      },
      "source": [
        "\n",
        "## Custom Layer Definitions\n",
        "\n",
        "Here we define custom layers with appropriate documentation and naming conventions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68055aff",
      "metadata": {
        "id": "68055aff"
      },
      "source": [
        "\n",
        "### BoolformerLayer\n",
        "\n",
        "This custom TensorFlow layer performs a logical AND operation on its input and then processes it through a dense layer with ReLU activation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ecebd4a8",
      "metadata": {
        "id": "ecebd4a8"
      },
      "outputs": [],
      "source": [
        "class BoolformerLayer(layers.Layer):\n",
        "    def __init__(self, threshold=0.5, **kwargs):\n",
        "        super(BoolformerLayer, self).__init__(**kwargs)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.dense_layer = layers.Dense(input_shape[-1], activation='relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        boolean_inputs = tf.greater(inputs, self.threshold)  # Convert to boolean based on threshold\n",
        "        logic_and = tf.math.logical_and(boolean_inputs, boolean_inputs)\n",
        "        return self.dense_layer(tf.cast(logic_and, tf.float32))  # Convert back to float\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d270c1e",
      "metadata": {
        "id": "3d270c1e"
      },
      "source": [
        "\n",
        "### QLearningLayer\n",
        "\n",
        "This layer is designed for reinforcement learning tasks, using a Q-learning algorithm to learn the quality of actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d99d45e7",
      "metadata": {
        "id": "d99d45e7"
      },
      "outputs": [],
      "source": [
        "class QLearningLayer(layers.Layer):\n",
        "    def __init__(self, action_space_size, learning_rate=0.01, gamma=0.95, **kwargs):\n",
        "        super(QLearningLayer, self).__init__(**kwargs)\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # A dense layer to process state and output Q-values for each action\n",
        "        self.dense = layers.Dense(self.action_space_size, activation=None)\n",
        "\n",
        "    def call(self, state, action=None, reward=None, next_state=None):\n",
        "        q_values = self.dense(state)\n",
        "\n",
        "        if action is not None and reward is not None and next_state is not None:\n",
        "            # Get the predicted Q-values for the next state\n",
        "            future_q_values = self.dense(next_state)\n",
        "            max_future_q = tf.reduce_max(future_q_values, axis=1)\n",
        "\n",
        "            # Compute the updated Q-value for the chosen action\n",
        "            q_update = reward + self.gamma * max_future_q\n",
        "            q_values_with_update = tf.tensor_scatter_nd_update(\n",
        "                q_values, tf.expand_dims(action, axis=-1), q_update)\n",
        "\n",
        "            # Update the Q-values\n",
        "            self.dense.set_weights([q_values_with_update])\n",
        "\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41391791",
      "metadata": {
        "id": "41391791"
      },
      "source": [
        "\n",
        "## Helper Functions\n",
        "\n",
        "Defining helper functions such as positional encoding and transformer encoder with detailed comments for better understanding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3709070",
      "metadata": {
        "id": "f3709070"
      },
      "source": [
        "\n",
        "### Positional Encoding Function\n",
        "\n",
        "Positional encoding adds information about the position of elements in the input sequence, crucial for models like transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f0616acf",
      "metadata": {
        "id": "f0616acf"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_length, d_model):\n",
        "    position = tf.range(seq_length, dtype=tf.float32)[:, tf.newaxis]\n",
        "    div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
        "\n",
        "    # Creating sine and cosine functions separately and then concatenating them\n",
        "    sine_terms = tf.sin(position * div_term)\n",
        "    cosine_terms = tf.cos(position * div_term)\n",
        "\n",
        "    # Interleaving sine and cosine terms\n",
        "    pos_encoding = tf.reshape(tf.concat([sine_terms, cosine_terms], axis=-1), [1, seq_length, d_model])\n",
        "\n",
        "    return pos_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1527ac5",
      "metadata": {
        "id": "a1527ac5"
      },
      "source": [
        "\n",
        "### Transformer Encoder Function\n",
        "\n",
        "The transformer encoder function applies transformations to the input data using layer normalization and multi-head attention, followed by a series of dense layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "fd3e2ba8",
      "metadata": {
        "id": "fd3e2ba8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(inputs.shape[-1])(x)\n",
        "    return x + res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4ed9bc",
      "metadata": {
        "id": "1f4ed9bc"
      },
      "source": [
        "\n",
        "## Model Building and Compilation\n",
        "\n",
        "Here we build and compile the neural network model, ensuring clarity and efficiency in the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5f6965",
      "metadata": {
        "id": "de5f6965"
      },
      "source": [
        "\n",
        "### Neural Network Model Creation Function\n",
        "\n",
        "This function constructs the neural network using the previously defined custom layers and functions. It integrates the transformer encoder with the custom `BoolformerLayer` and `QLearningLayer`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7a3619fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a3619fa",
        "outputId": "a98a30cc-187e-4186-883e-8eb8d0866307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 128, 512)]           0         []                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TF  (None, 128, 512)             0         ['input_5[0][0]']             \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 128, 512)             1024      ['tf.__operators__.add_7[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 128, 512)             131776    ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 128, 512)             0         ['multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TF  (None, 128, 512)             0         ['dropout_4[0][0]',           \n",
            " OpLambda)                                                           'tf.__operators__.add_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 128, 512)             1024      ['tf.__operators__.add_8[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 128, 64)              32832     ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 128, 64)              0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 128, 512)             33280     ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TF  (None, 128, 512)             0         ['dense_5[0][0]',             \n",
            " OpLambda)                                                           'tf.__operators__.add_8[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " boolformer_layer_2 (Boolfo  (None, 128, 512)             262656    ['tf.__operators__.add_9[0][0]\n",
            " rmerLayer)                                                         ']                            \n",
            "                                                                                                  \n",
            " q_learning_layer_2 (QLearn  (None, 128, 10)              5130      ['boolformer_layer_2[0][0]']  \n",
            " ingLayer)                                                                                        \n",
            "                                                                                                  \n",
            " Output (Dense)              (None, 128, 10)              110       ['q_learning_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " Reward (Dense)              (None, 128, 1)               11        ['q_learning_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 467843 (1.78 MB)\n",
            "Trainable params: 467843 (1.78 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def create_neural_network_model():\n",
        "    input_layer = keras.Input(shape=(seq_length, d_model))\n",
        "\n",
        "    # Generate positional encoding and add it to the input\n",
        "    pos_encoding = positional_encoding(seq_length, d_model)  # Ensure this returns a tensor\n",
        "    pos_encoded = input_layer + pos_encoding\n",
        "\n",
        "    # Transformer encoder\n",
        "    transformer_output = transformer_encoder(inputs=pos_encoded, head_size=32, num_heads=2, ff_dim=64)\n",
        "\n",
        "    # Custom layers (assuming these are correctly defined elsewhere)\n",
        "    x_bool = BoolformerLayer()(transformer_output)\n",
        "    rl_layer = QLearningLayer(action_space_size=num_classes)(x_bool)\n",
        "\n",
        "    # Output layers\n",
        "    output_layer = layers.Dense(num_classes, activation='softmax', name='Output')(rl_layer)\n",
        "    reward_layer = layers.Dense(1, name='Reward')(rl_layer)\n",
        "\n",
        "    # Constructing the model\n",
        "    model = keras.Model(inputs=input_layer, outputs=[output_layer, reward_layer])\n",
        "\n",
        "    # Compiling the model\n",
        "    opt = optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss={'Output': 'categorical_crossentropy', 'Reward': 'mean_squared_error'},\n",
        "                  metrics={'Output': 'accuracy'})\n",
        "\n",
        "    return model\n",
        "\n",
        "# Creating the model\n",
        "model = create_neural_network_model()\n",
        "\n",
        "# Displaying the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ca1a1f",
      "metadata": {
        "id": "f7ca1a1f"
      },
      "source": [
        "\n",
        "## Visualizing Model Performance\n",
        "\n",
        "Functions for plotting and analyzing the model's performance during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d2c85c",
      "metadata": {
        "id": "65d2c85c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_model_performance(history):\n",
        "    # Plotting accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb2d5ffc",
      "metadata": {
        "id": "eb2d5ffc"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook provided a detailed walkthrough for developing, training, and evaluating a neural network model with custom layers and advanced techniques, ensuring good coding practices and clear documentation throughout.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}